{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k=3, distance_metric='euclidean'):\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Store the training data as NumPy arrays\n",
    "        self.X_train = np.array(X)\n",
    "        self.y_train = np.array(y, dtype=int)  # Ensure y_train is integer type\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predict the label for each example in the test set using vectorized operations\n",
    "        predictions = [self._predict(x) for x in np.array(X)]\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        # Compute the distances between x and all examples in the training set\n",
    "        distances = self.compute_distances(x, self.X_train)\n",
    "        \n",
    "        # Sort by distance and return the indices of the k nearest neighbors\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        \n",
    "        # Get the labels of the k nearest samples\n",
    "        k_nearest_labels = self.y_train[k_indices]\n",
    "        \n",
    "        # Return the most common class among the k neighbors\n",
    "        return np.bincount(k_nearest_labels).argmax()\n",
    "\n",
    "    def compute_distances(self, x, X_train):\n",
    "        \"\"\" Efficient vectorized distance computation. \"\"\"\n",
    "        if self.distance_metric == 'euclidean':\n",
    "            # Vectorized Euclidean distance computation\n",
    "            return np.sqrt(np.sum((X_train - x) ** 2, axis=1))\n",
    "        elif self.distance_metric == 'manhattan':\n",
    "            # Vectorized Manhattan distance computation\n",
    "            return np.sum(np.abs(X_train - x), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def z_score_scaler(X, mean=None, std=None):\n",
    "    \"\"\" Scale features using z-score normalization. \"\"\"\n",
    "    if mean is None:\n",
    "        mean = np.mean(X, axis=0)\n",
    "    if std is None:\n",
    "        std = np.std(X, axis=0)\n",
    "    return (X - mean) / std, mean, std\n",
    "\n",
    "def one_hot_encode(df, column):\n",
    "    \"\"\" One-hot encode a single categorical column. \"\"\"\n",
    "    dummies = pd.get_dummies(df[column], prefix=column, drop_first=True)\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    df.drop(column, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def preprocess_data(train_path, test_path):\n",
    "    \"\"\" Preprocess the train and test datasets according to the task requirements. \"\"\"\n",
    "    \n",
    "    # Load the datasets\n",
    "    train_data = pd.read_csv(train_path)\n",
    "    test_data = pd.read_csv(test_path)\n",
    "    \n",
    "    # Drop irrelevant columns (CustomerId, Surname, id)\n",
    "    train_data = train_data.drop(columns=[\"CustomerId\", \"Surname\", \"id\"])\n",
    "    test_data = test_data.drop(columns=[\"CustomerId\", \"Surname\", \"id\"])\n",
    "    \n",
    "    # Handle missing values for numerical columns (fill with median)\n",
    "    # Selecting only numerical columns for missing value imputation\n",
    "    numeric_columns_train = train_data.select_dtypes(include=[np.number]).columns.drop(\"Exited\")\n",
    "    numeric_columns_test = test_data.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    # Fill missing values with median for numeric columns\n",
    "    train_data[numeric_columns_train] = train_data[numeric_columns_train].fillna(train_data[numeric_columns_train].median())\n",
    "    test_data[numeric_columns_test] = test_data[numeric_columns_test].fillna(test_data[numeric_columns_test].median())\n",
    "    \n",
    "    # One-hot encode categorical variables (Geography, Gender) for both train and test data\n",
    "    train_data = one_hot_encode(train_data, \"Geography\")\n",
    "    train_data = one_hot_encode(train_data, \"Gender\")\n",
    "    \n",
    "    test_data = one_hot_encode(test_data, \"Geography\")\n",
    "    test_data = one_hot_encode(test_data, \"Gender\")\n",
    "    \n",
    "    # Align test_data with train_data columns by using column names (excluding 'Exited')\n",
    "    # We don't want to use row indices for reindexing columns, so we use train_data's columns except for 'Exited'.\n",
    "    test_data = test_data.reindex(columns=train_data.drop(columns=[\"Exited\"]).columns, fill_value=0)\n",
    "    \n",
    "    # Separate features (X) and target (y) for training data\n",
    "    X_train = train_data.drop(columns=[\"Exited\"])  # Training features\n",
    "    y_train = train_data[\"Exited\"]\n",
    "    \n",
    "    # Scale the features using z-score normalization (fit on train, transform on both train and test)\n",
    "    X_train_scaled, mean_train, std_train = z_score_scaler(X_train)\n",
    "    X_test_scaled, _, _ = z_score_scaler(test_data, mean_train, std_train)\n",
    "    \n",
    "    return X_train_scaled, y_train, X_test_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def accuracy_score_manual(y_true, y_pred):\n",
    "    \"\"\" Manually calculate accuracy. \"\"\"\n",
    "    return np.sum(y_true == y_pred) / len(y_true)\n",
    "\n",
    "def precision_score_manual(y_true, y_pred):\n",
    "    \"\"\" Manually calculate precision. \"\"\"\n",
    "    true_positives = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    predicted_positives = np.sum(y_pred == 1)\n",
    "    if predicted_positives == 0:\n",
    "        return 0\n",
    "    return true_positives / predicted_positives\n",
    "\n",
    "def recall_score_manual(y_true, y_pred):\n",
    "    \"\"\" Manually calculate recall. \"\"\"\n",
    "    true_positives = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    actual_positives = np.sum(y_true == 1)\n",
    "    if actual_positives == 0:\n",
    "        return 0\n",
    "    return true_positives / actual_positives\n",
    "\n",
    "def f1_score_manual(y_true, y_pred):\n",
    "    \"\"\" Manually calculate F1 score. \"\"\"\n",
    "    precision = precision_score_manual(y_true, y_pred)\n",
    "    recall = recall_score_manual(y_true, y_pred)\n",
    "    if (precision + recall) == 0:\n",
    "        return 0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "def roc_auc_score_manual(y_true, y_pred):\n",
    "    \"\"\" Manually calculate ROC AUC score. \"\"\"\n",
    "    # For simplicity, we'll approximate ROC AUC using a basic method:\n",
    "    # Here, we treat it as the proportion of correct rankings of positive vs. negative instances\n",
    "    pos = y_true == 1\n",
    "    neg = y_true == 0\n",
    "    correct_rankings = 0\n",
    "    total_pairs = np.sum(pos) * np.sum(neg)\n",
    "    \n",
    "    if total_pairs == 0:\n",
    "        return 0.5  # No positive or no negative examples\n",
    "\n",
    "    for i in range(len(y_true)):\n",
    "        if pos[i]:\n",
    "            correct_rankings += np.sum(y_pred[i] > y_pred[neg])\n",
    "\n",
    "    return correct_rankings / total_pairs\n",
    "\n",
    "def cross_validate(X, y, knn, n_splits=5):\n",
    "    # Shuffle and split the data into k folds\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    fold_size = X.shape[0] // n_splits\n",
    "    folds = [indices[i * fold_size: (i + 1) * fold_size] for i in range(n_splits)]\n",
    "    \n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    roc_auc_scores = []\n",
    "    \n",
    "    # Cross-validation loop\n",
    "    for i in range(n_splits):\n",
    "        # Create training and validation sets\n",
    "        val_indices = folds[i]\n",
    "        train_indices = np.concatenate([folds[j] for j in range(n_splits) if j != i])\n",
    "        \n",
    "        # Use .iloc[] to access rows by index for DataFrames\n",
    "        X_train, X_val = X.iloc[train_indices], X.iloc[val_indices]\n",
    "        y_train, y_val = y.iloc[train_indices], y.iloc[val_indices]\n",
    "        \n",
    "        # Train the KNN model on the training set\n",
    "        knn.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on the validation set\n",
    "        y_pred = knn.predict(X_val)\n",
    "        \n",
    "        # Calculate evaluation metrics manually\n",
    "        accuracy_scores.append(accuracy_score_manual(y_val.values, y_pred))\n",
    "        precision_scores.append(precision_score_manual(y_val.values, y_pred))\n",
    "        recall_scores.append(recall_score_manual(y_val.values, y_pred))\n",
    "        f1_scores.append(f1_score_manual(y_val.values, y_pred))\n",
    "        roc_auc_scores.append(roc_auc_score_manual(y_val.values, y_pred))\n",
    "    \n",
    "    # Return the average of each metric across the folds\n",
    "    return {\n",
    "        'accuracy': np.mean(accuracy_scores),\n",
    "        'precision': np.mean(precision_scores),\n",
    "        'recall': np.mean(recall_scores),\n",
    "        'f1': np.mean(f1_scores),\n",
    "        'roc_auc': np.mean(roc_auc_scores)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Scores for k=3, distance_metric=euclidean: {'accuracy': np.float64(0.8650666666666667), 'precision': np.float64(0.6992008952672787), 'recall': np.float64(0.5840197743926948), 'f1': np.float64(0.6363492802373804), 'roc_auc': np.float64(0.5467502909563609)}\n",
      "CV Scores for k=3, distance_metric=manhattan: {'accuracy': np.float64(0.8662666666666666), 'precision': np.float64(0.7034108107694647), 'recall': np.float64(0.5870844541841933), 'f1': np.float64(0.6395356336282887), 'roc_auc': np.float64(0.5502196735929143)}\n",
      "CV Scores for k=5, distance_metric=euclidean: {'accuracy': np.float64(0.8715333333333334), 'precision': np.float64(0.728312018396238), 'recall': np.float64(0.5820775727776403), 'f1': np.float64(0.6469771511494079), 'roc_auc': np.float64(0.5500358591253508)}\n",
      "CV Scores for k=5, distance_metric=manhattan: {'accuracy': np.float64(0.8719999999999999), 'precision': np.float64(0.7294563216491343), 'recall': np.float64(0.5837073503398408), 'f1': np.float64(0.6483667822375415), 'roc_auc': np.float64(0.5516689652848561)}\n",
      "CV Scores for k=7, distance_metric=euclidean: {'accuracy': np.float64(0.8744666666666667), 'precision': np.float64(0.7435756942496174), 'recall': np.float64(0.5795028069232546), 'f1': np.float64(0.6511105849677716), 'roc_auc': np.float64(0.550077701871994)}\n",
      "CV Scores for k=7, distance_metric=manhattan: {'accuracy': np.float64(0.8780666666666667), 'precision': np.float64(0.7506344913097932), 'recall': np.float64(0.5950695062745923), 'f1': np.float64(0.6637123777145731), 'roc_auc': np.float64(0.5651558296164327)}\n",
      "CV Scores for k=9, distance_metric=euclidean: {'accuracy': np.float64(0.8768666666666668), 'precision': np.float64(0.7558782312311654), 'recall': np.float64(0.578561303019635), 'f1': np.float64(0.6553796706836809), 'roc_auc': np.float64(0.5511917319842696)}\n",
      "CV Scores for k=9, distance_metric=manhattan: {'accuracy': np.float64(0.8783333333333333), 'precision': np.float64(0.7608183875750621), 'recall': np.float64(0.5837981364339573), 'f1': np.float64(0.6597948933245703), 'roc_auc': np.float64(0.5563237518180186)}\n",
      "CV Scores for k=11, distance_metric=euclidean: {'accuracy': np.float64(0.8764666666666667), 'precision': np.float64(0.7588528945460281), 'recall': np.float64(0.5703791588770744), 'f1': np.float64(0.6510337105027537), 'roc_auc': np.float64(0.5441567163409531)}\n",
      "CV Scores for k=11, distance_metric=manhattan: {'accuracy': np.float64(0.8777333333333333), 'precision': np.float64(0.7626920541264498), 'recall': np.float64(0.574191278146049), 'f1': np.float64(0.6550101357098214), 'roc_auc': np.float64(0.5481688204630858)}\n",
      "Best hyperparameters: k=9, distance_metric=manhattan\n",
      "Submission file saved as 'submissions.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "X, y, X_test = preprocess_data('train.csv', 'test.csv')\n",
    "\n",
    "# Hyperparameter tuning (finding the best k and distance metric)\n",
    "best_k = None\n",
    "best_distance_metric = None\n",
    "best_cv_score = -1\n",
    "\n",
    "# Define possible values for hyperparameters\n",
    "k_values = [3, 5, 7, 9, 11]\n",
    "distance_metrics = ['euclidean', 'manhattan']\n",
    "\n",
    "# Try each combination of k and distance metric\n",
    "for k in k_values:\n",
    "    for distance_metric in distance_metrics:\n",
    "        knn = KNN(k=k, distance_metric=distance_metric)\n",
    "        cv_scores = cross_validate(X, y, knn)\n",
    "        print(f\"CV Scores for k={k}, distance_metric={distance_metric}: {cv_scores}\")\n",
    "        \n",
    "        # Check if this is the best model so far\n",
    "        if cv_scores['accuracy'] > best_cv_score:\n",
    "            best_cv_score = cv_scores['accuracy']\n",
    "            best_k = k\n",
    "            best_distance_metric = distance_metric\n",
    "\n",
    "print(f\"Best hyperparameters: k={best_k}, distance_metric={best_distance_metric}\")\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "knn = KNN(k=best_k, distance_metric=best_distance_metric)\n",
    "knn.fit(X, y)\n",
    "\n",
    "# Predict on the test set\n",
    "test_predictions = knn.predict(X_test)\n",
    "\n",
    "# Save the test predictions\n",
    "submission = pd.DataFrame({'id': pd.read_csv('test.csv')['id'], 'Exited': test_predictions})\n",
    "submission.to_csv('submissions.csv', index=False)\n",
    "\n",
    "print(\"Submission file saved as 'submissions.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
